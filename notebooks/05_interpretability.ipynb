{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5: Model Interpretability\n",
    "\n",
    "This notebook handles:\n",
    "- SHAP explanations\n",
    "- Difficult case analysis\n",
    "- Model transparency reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from transformers import pipeline\n",
    "from src.interpretability.model_explainer import NERExplainer\n",
    "from src.labeling.conll_labeler import CoNLLLabeler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using best model: xlm-roberta-base\n",
      "Model path: ../models/checkpoints/xlm-roberta-base\n"
     ]
    }
   ],
   "source": [
    "# Load best model (from evaluation results)\n",
    "results_df = pd.read_csv(\"../models/comparison_results.csv\")\n",
    "best_model_name = results_df.iloc[0]['model']\n",
    "model_path = f\"../models/checkpoints/{best_model_name.replace('/', '_')}\"\n",
    "\n",
    "print(f\"Using best model: {best_model_name}\")\n",
    "print(f\"Model path: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and explainer initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize model pipeline and explainer\n",
    "if Path(model_path).exists():\n",
    "    model_pipeline = pipeline('ner', model=model_path, aggregation_strategy='simple')\n",
    "    explainer = NERExplainer(model_pipeline)\n",
    "    print(\"Model and explainer initialized\")\n",
    "else:\n",
    "    print(f\"Model not found at {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text: áˆ»áŠ•áŒ£ á‹‹áŒ‹ 500 á‰¥áˆ­ áŠ á‹²áˆµ áŠ á‰ á‰£ á‰¦áˆŒ\n",
      "Predictions:\n",
      "\n",
      "Text: áˆµáˆáŠ­ á‰  2000 á‰¥áˆ­ áˆ˜áˆ­áŠ«á‰¶ áˆ‹á‹­\n",
      "Predictions:\n",
      "\n",
      "Text: áŒ«áˆ› áŠ¥áŠ“ áˆá‰¥áˆµ á’á‹«áˆ³ á‹áˆµáŒ¥\n",
      "Predictions:\n"
     ]
    }
   ],
   "source": [
    "# Test predictions on sample texts\n",
    "sample_texts = [\n",
    "    \"áˆ»áŠ•áŒ£ á‹‹áŒ‹ 500 á‰¥áˆ­ áŠ á‹²áˆµ áŠ á‰ á‰£ á‰¦áˆŒ\",\n",
    "    \"áˆµáˆáŠ­ á‰  2000 á‰¥áˆ­ áˆ˜áˆ­áŠ«á‰¶ áˆ‹á‹­\",\n",
    "    \"áŒ«áˆ› áŠ¥áŠ“ áˆá‰¥áˆµ á’á‹«áˆ³ á‹áˆµáŒ¥\"\n",
    "]\n",
    "\n",
    "if 'model_pipeline' in locals():\n",
    "    for text in sample_texts:\n",
    "        predictions = model_pipeline(text)\n",
    "        print(f\"\\nText: {text}\")\n",
    "        print(\"Predictions:\")\n",
    "        for pred in predictions:\n",
    "            print(f\"  {pred['word']} -> {pred['entity_group']} ({pred['score']:.3f})\")\n",
    "else:\n",
    "    print(\"Model pipeline not initialized. Run previous cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Explaining: áˆ»áŠ•áŒ£ á‹‹áŒ‹ 500 á‰¥áˆ­ áŠ á‹²áˆµ áŠ á‰ á‰£ á‰¦áˆŒ\n",
      "SHAP values:\n",
      "  O: 0.250\n",
      "  Product: 0.250\n",
      "  Location: 0.250\n",
      "  Price: 0.250\n",
      "\n",
      "Explaining: áˆµáˆáŠ­ á‰  2000 á‰¥áˆ­ áˆ˜áˆ­áŠ«á‰¶ áˆ‹á‹­\n",
      "SHAP values:\n",
      "  O: 0.250\n",
      "  Product: 0.250\n",
      "  Location: 0.250\n",
      "  Price: 0.250\n"
     ]
    }
   ],
   "source": [
    "# Generate SHAP explanations\n",
    "if 'explainer' in locals():\n",
    "    for text in sample_texts[:2]:\n",
    "        print(f\"\\nExplaining: {text}\")\n",
    "        \n",
    "        explanation = explainer.explain_with_shap(text)\n",
    "        \n",
    "        if explanation and 'shap_values' in explanation:\n",
    "            print(\"SHAP values:\")\n",
    "            shap_vals = explanation['shap_values']\n",
    "            labels = ['O', 'Product', 'Location', 'Price']\n",
    "            for i, val in enumerate(shap_vals):\n",
    "                print(f\"  {labels[i]}: {val:.3f}\")\n",
    "        else:\n",
    "            print(\"  No explanation generated\")\n",
    "else:\n",
    "    print(\"Explainer not initialized. Run previous cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difficult Cases Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing 50 examples for difficult cases\n"
     ]
    }
   ],
   "source": [
    "# Load test data for difficult case analysis\n",
    "labeler = CoNLLLabeler()\n",
    "with open(\"../data/labeled/train_data.conll\", 'r', encoding='utf-8') as f:\n",
    "    conll_data = f.read()\n",
    "\n",
    "test_data = labeler.parse_conll_data(conll_data)\n",
    "print(f\"Analyzing {len(test_data)} examples for difficult cases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 difficult cases\n",
      "\n",
      "Difficult Case 1:\n",
      "Text: ğŸ’¥ğŸ’¥...................................ğŸ’¥ğŸ’¥ â“ á‰ áˆ¨áá‰µ á‰€áŠ•á‹ áˆ±á‰… áˆ‹á‹­ áˆ˜áˆµá‰°áŠ“áŒˆá‹µ áˆˆáˆá‰µáˆáˆáŒ‰ á‹á‹µ á‹°áŠ•á‰ áŠá‰»á‰½áŠ• ğŸŒáŠáŒˆ áŠ¨áŒ á‹‹á‰± 4:30 _ á‰€áŠ‘...\n",
      "Issues: ['Model over-predicting entities', \"Entity type mismatch: true=set(), pred={'PRICE'}\"]\n",
      "Predictions: 6 entities\n",
      "\n",
      "Difficult Case 2:\n",
      "Text: ğŸ“Œ áˆˆ1 áˆ³áˆáŠ•á‰µ á‹¨áˆšá‰†á‹­ ğŸ¯ NovaÂ®ğŸ‘ŸğŸ‘ŸğŸ‘šğŸ‘” áŠ áŠáˆµá‰°áŠ› á‹¨áˆá‰¥áˆµ áŠ¥áŠ“ á‹¨áŒ«áˆ› áˆ›áŒ á‰¢á‹« áˆ›áˆ½áŠ• ğŸ‘ŸğŸ‘”ğŸ©³ğŸ‘Ÿ ğŸ”° áˆˆá‰²áˆ¸áˆ­á‰µá¡ áˆˆáŒ‚áŠ•áˆµ á¡ áˆˆáˆ¹áˆ«á‰¥á¡áˆˆáŠ áŠ•áˆ¶áˆ‹ á£áŒ«áˆ› ğŸ”°á‰ áŒ á‰…áˆ‹áˆ‹á‹ áŠ¥áˆµ...\n",
      "Issues: [\"Entity type mismatch: true={'PRICE', 'Product'}, pred={'PRICE'}\"]\n",
      "Predictions: 2 entities\n",
      "\n",
      "Difficult Case 3:\n",
      "Text: **ORGANIC EXTRA OLIVE OIL 2L Price 6500 birr Telegram á¡-******** MsgğŸ‘‰ Lobelia pharmacy and cosmetics...\n",
      "Issues: ['Model over-predicting entities', \"Entity type mismatch: true={'PRICE'}, pred={'LOC', 'PRICE'}\"]\n",
      "Predictions: 9 entities\n"
     ]
    }
   ],
   "source": [
    "# Analyze difficult cases\n",
    "difficult_cases = explainer.analyze_difficult_cases(test_data[:20])\n",
    "\n",
    "print(f\"Found {len(difficult_cases)} difficult cases\")\n",
    "\n",
    "for i, case in enumerate(difficult_cases[:3]):\n",
    "    print(f\"\\nDifficult Case {i+1}:\")\n",
    "    print(f\"Text: {case['text'][:100]}...\")\n",
    "    print(f\"Issues: {case['issues']}\")\n",
    "    print(f\"Predictions: {len(case['predictions'])} entities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Interpretability Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpretability report saved to models/interpretability_report.json\n"
     ]
    }
   ],
   "source": [
    "# Generate comprehensive report\n",
    "explainer.generate_interpretability_report(test_data[:20], \"../models/interpretability_report.json\")\n",
    "\n",
    "print(\"Interpretability report saved to models/interpretability_report.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Report Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpretability Report Summary:\n",
      "Cases analyzed: 20\n",
      "Difficult cases: 8\n",
      "\n",
      "Common Issues:\n",
      "  Model over-predicting entities: 4\n",
      "  Entity type mismatch: true={'PRICE'}, pred={'LOC', 'PRICE'}: 2\n",
      "  Model failed to detect any entities: 2\n",
      "  Entity type mismatch: true={'Product'}, pred=set(): 2\n",
      "  Entity type mismatch: true={'LOC', 'PRICE'}, pred={'PRICE'}: 2\n",
      "  Entity type mismatch: true=set(), pred={'PRICE'}: 1\n",
      "  Entity type mismatch: true={'PRICE', 'Product'}, pred={'PRICE'}: 1\n",
      "\n",
      "Recommendations:\n",
      "  - Consider increasing model sensitivity or adding more training data with similar patterns.\n",
      "  - Consider adjusting confidence thresholds or adding negative examples to training data.\n",
      "  - Review entity type definitions and add more diverse examples for each entity type.\n",
      "  - Consider data augmentation techniques for underrepresented entity patterns.\n",
      "\n",
      "Model interpretability analysis completed!\n"
     ]
    }
   ],
   "source": [
    "# Load and display report summary\n",
    "import json\n",
    "\n",
    "with open(\"../models/interpretability_report.json\", 'r', encoding='utf-8') as f:\n",
    "    report = json.load(f)\n",
    "\n",
    "print(\"Interpretability Report Summary:\")\n",
    "print(f\"Cases analyzed: {report['summary']['total_cases_analyzed']}\")\n",
    "print(f\"Difficult cases: {report['summary']['difficult_cases_found']}\")\n",
    "\n",
    "print(\"\\nCommon Issues:\")\n",
    "for issue, count in report['summary']['common_issues'].items():\n",
    "    print(f\"  {issue}: {count}\")\n",
    "\n",
    "print(\"\\nRecommendations:\")\n",
    "for rec in report['recommendations']:\n",
    "    print(f\"  - {rec}\")\n",
    "\n",
    "print(\"\\nModel interpretability analysis completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
