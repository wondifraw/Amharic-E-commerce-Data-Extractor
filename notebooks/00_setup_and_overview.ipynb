{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amharic NER Project Setup & Overview\n",
    "\n",
    "This notebook provides:\n",
    "- Project setup and environment configuration\n",
    "- Overview of all tasks\n",
    "- Quick pipeline execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check environment\n",
    "import sys\n",
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Transformers: {transformers.__version__}\")\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "\n",
    "# Check GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"GPU: Not available (using CPU)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Structure:\n",
      "├── config\n",
      "│   └── config.py\n",
      "├── data\n",
      "│   ├── labeled\n",
      "│   ├── processed\n",
      "│   └── raw\n",
      "├── main_pipeline.py\n",
      "├── models\n",
      "│   └── checkpoints\n",
      "├── notebooks\n",
      "│   ├── 00_setup_and_overview.ipynb\n",
      "│   ├── 01_data_ingestion.ipynb\n",
      "│   ├── 02_conll_labeling.ipynb\n",
      "│   ├── 03_model_training.ipynb\n",
      "│   ├── 04_model_evaluation.ipynb\n",
      "│   ├── 05_interpretability.ipynb\n",
      "│   └── 06_vendor_analytics.ipynb\n",
      "├── README.md\n",
      "├── requirements.txt\n",
      "├── scripts\n",
      "│   └── run_individual_tasks.py\n",
      "├── src\n",
      "│   ├── __init__.py\n",
      "│   ├── data_ingestion\n",
      "│   │   └── telegram_scraper.py\n",
      "│   ├── evaluation\n",
      "│   │   └── model_evaluator.py\n",
      "│   ├── interpretability\n",
      "│   │   └── model_explainer.py\n",
      "│   ├── labeling\n",
      "│   │   └── conll_labeler.py\n",
      "│   ├── preprocessing\n",
      "│   │   └── text_processor.py\n",
      "│   ├── training\n",
      "│   │   └── ner_trainer.py\n",
      "│   ├── utils\n",
      "│   │   ├── __init__.py\n",
      "│   │   └── etnltk_helper.py\n",
      "│   └── vendor_analytics\n",
      "│       └── scorecard.py\n",
      "└── tests\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Display project structure\n",
    "def show_tree(path, prefix=\"\", max_depth=3, current_depth=0):\n",
    "    if current_depth > max_depth:\n",
    "        return\n",
    "    \n",
    "    path = Path(path)\n",
    "    items = sorted([p for p in path.iterdir() if not p.name.startswith('.')])\n",
    "    \n",
    "    for i, item in enumerate(items):\n",
    "        is_last = i == len(items) - 1\n",
    "        current_prefix = \"└── \" if is_last else \"├── \"\n",
    "        print(f\"{prefix}{current_prefix}{item.name}\")\n",
    "        \n",
    "        if item.is_dir() and current_depth < max_depth:\n",
    "            next_prefix = prefix + (\"    \" if is_last else \"│   \")\n",
    "            show_tree(item, next_prefix, max_depth, current_depth + 1)\n",
    "\n",
    "print(\"Project Structure:\")\n",
    "show_tree(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Overview\n",
    "\n",
    "### Task 1: Data Ingestion & Preprocessing\n",
    "- Scrape 5+ Ethiopian Telegram e-commerce channels\n",
    "- Clean and preprocess Amharic text using etnltk\n",
    "- Extract entity hints and metadata\n",
    "\n",
    "### Task 2: CoNLL Labeling\n",
    "- Auto-label 30-50 messages in CoNLL format\n",
    "- Entity types: Product, Location, Price\n",
    "- BIO tagging scheme\n",
    "\n",
    "### Task 3: Model Training\n",
    "- Fine-tune XLM-RoBERTa, DistilBERT, mBERT\n",
    "- Use Hugging Face Transformers\n",
    "- GPU-accelerated training\n",
    "\n",
    "### Task 4: Model Evaluation\n",
    "- Compare models on accuracy, speed, robustness\n",
    "- Select best performing model\n",
    "- Generate comparison reports\n",
    "\n",
    "### Task 5: Interpretability\n",
    "- SHAP and LIME explanations\n",
    "- Difficult case analysis\n",
    "- Model transparency reports\n",
    "\n",
    "### Task 6: Vendor Analytics\n",
    "- Calculate vendor performance metrics\n",
    "- Generate micro-lending scorecards\n",
    "- Business insights and recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "Target Channels: 9\n",
      "Models to Train: 3\n",
      "Entity Labels: ['O', 'B-Product', 'I-Product', 'B-LOC', 'I-LOC', 'B-PRICE', 'I-PRICE']\n",
      "✅ Environment file configured\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from config.config import telegram_config, model_config, data_config\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"Target Channels: {len(telegram_config.channels)}\")\n",
    "print(f\"Models to Train: {len(model_config.model_names)}\")\n",
    "print(f\"Entity Labels: {data_config.entity_labels}\")\n",
    "\n",
    "# Check if .env file exists\n",
    "env_file = Path(\"../.env\")\n",
    "if env_file.exists():\n",
    "    print(\"✅ Environment file configured\")\n",
    "else:\n",
    "    print(\"⚠️  Please copy .env.example to .env and configure Telegram API credentials\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Pipeline Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "etnltk tokenization failed: cannot import name 'word_tokenize' from 'etnltk.tokenize' (c:\\Users\\W-HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\etnltk\\tokenize\\__init__.py)\n",
      "etnltk tokenization failed: cannot import name 'word_tokenize' from 'etnltk.tokenize' (c:\\Users\\W-HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\etnltk\\tokenize\\__init__.py)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ሻንጣ ዋጋ 500 ብር አዲስ አበባ ቦሌ ላይ\n",
      "Cleaned: ሻንጣ ዋጋ 500 ብር አዲስ አበባ ቦሌ ላይ\n",
      "Tokens: ['ሻንጣ', 'ዋጋ', '500', 'ብር', 'አዲስ', 'አበባ', 'ቦሌ', 'ላይ']\n",
      "\n",
      "Labeled tokens:\n",
      "  ሻንጣ -> B-Product\n",
      "  ዋጋ -> I-Product\n",
      "  500 -> B-PRICE\n",
      "  ብር -> I-PRICE\n",
      "  አዲስ -> B-LOC\n",
      "  አበባ -> I-LOC\n",
      "  ቦሌ -> B-LOC\n",
      "  ላይ -> O\n",
      "\n",
      "✅ Core components working correctly!\n"
     ]
    }
   ],
   "source": [
    "# Test core components\n",
    "from src.preprocessing.text_processor import AmharicTextProcessor\n",
    "from src.labeling.conll_labeler import CoNLLLabeler\n",
    "from src.vendor_analytics.scorecard import VendorAnalytics\n",
    "\n",
    "# Test text processing\n",
    "processor = AmharicTextProcessor()\n",
    "sample_text = \"ሻንጣ ዋጋ 500 ብር አዲስ አበባ ቦሌ ላይ\"\n",
    "cleaned = processor.clean_text(sample_text)\n",
    "tokens = processor.tokenize_amharic(sample_text)\n",
    "\n",
    "print(f\"Original: {sample_text}\")\n",
    "print(f\"Cleaned: {cleaned}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "\n",
    "# Test labeling\n",
    "labeler = CoNLLLabeler()\n",
    "labeled = labeler.auto_label_message(sample_text)\n",
    "print(f\"\\nLabeled tokens:\")\n",
    "for token, label in labeled:\n",
    "    print(f\"  {token} -> {label}\")\n",
    "\n",
    "print(\"\\n✅ Core components working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Configure Environment**: Set up `.env` file with Telegram API credentials\n",
    "2. **Run Task 1**: Execute `01_data_ingestion.ipynb` to scrape and preprocess data\n",
    "3. **Run Task 2**: Execute `02_conll_labeling.ipynb` to create labeled dataset\n",
    "4. **Run Task 3**: Execute `03_model_training.ipynb` to train NER models\n",
    "5. **Run Task 4**: Execute `04_model_evaluation.ipynb` to compare models\n",
    "6. **Run Task 5**: Execute `05_interpretability.ipynb` for model explanations\n",
    "7. **Run Task 6**: Execute `06_vendor_analytics.ipynb` for business insights\n",
    "\n",
    "**Alternative**: Run `python main_pipeline.py` to execute all tasks automatically."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
