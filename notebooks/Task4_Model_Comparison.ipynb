{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Model Comparison & Selection\n",
    "Comparing multiple NER models and selecting the best performer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from ner.model_trainer import ModelComparator\n",
    "from evaluation.model_evaluator import NERModelEvaluator\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model comparator\n",
    "comparator = ModelComparator()\n",
    "print(\"Model comparator initialized\")\n",
    "print(f\"Models to compare: {comparator.config['ner']['model_names']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data splits\n",
    "train_data_path = \"../data/labeled/ethiopian_ner_dataset_train.txt\"\n",
    "eval_data_path = \"../data/labeled/ethiopian_ner_dataset_eval.txt\"\n",
    "\n",
    "# Create train/eval splits from main dataset\n",
    "from ner.model_trainer import NERModelTrainer\n",
    "trainer = NERModelTrainer()\n",
    "sentences, labels = trainer.load_conll_data(\"../data/labeled/ethiopian_ner_dataset.txt\")\n",
    "\n",
    "split_idx = int(0.8 * len(sentences))\n",
    "train_sentences = sentences[:split_idx]\n",
    "train_labels = labels[:split_idx]\n",
    "eval_sentences = sentences[split_idx:]\n",
    "eval_labels = labels[split_idx:]\n",
    "\n",
    "print(f\"Training: {len(train_sentences)} sentences\")\n",
    "print(f\"Evaluation: {len(eval_sentences)} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save split data\n",
    "import os\n",
    "os.makedirs(\"../data/labeled\", exist_ok=True)\n",
    "\n",
    "# Save training data\n",
    "with open(train_data_path, 'w', encoding='utf-8') as f:\n",
    "    for sent, labs in zip(train_sentences, train_labels):\n",
    "        for token, label in zip(sent, labs):\n",
    "            f.write(f\"{token}\\t{label}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "# Save evaluation data\n",
    "with open(eval_data_path, 'w', encoding='utf-8') as f:\n",
    "    for sent, labs in zip(eval_sentences, eval_labels):\n",
    "        for token, label in zip(sent, labs):\n",
    "            f.write(f\"{token}\\t{label}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(\"Data splits saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "print(\"Starting model comparison...\")\n",
    "results = comparator.compare_models(train_data_path, eval_data_path)\n",
    "print(\"Model comparison completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comparison results\n",
    "print(\"Model Comparison Results:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Model':<30} {'F1 Score':<12} {'Accuracy':<12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for model_name, result in results.items():\n",
    "    if 'error' not in result:\n",
    "        f1 = result.get('eval_f1', 0)\n",
    "        acc = result.get('eval_accuracy', 0)\n",
    "        print(f\"{model_name:<30} {f1:<12.3f} {acc:<12.3f}\")\n",
    "    else:\n",
    "        print(f\"{model_name:<30} {'ERROR':<12} {'ERROR':<12}\")\n",
    "\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model\n",
    "best_model = comparator.get_best_model()\n",
    "print(f\"Best performing model: {best_model}\")\n",
    "\n",
    "if best_model and best_model in results:\n",
    "    best_result = results[best_model]\n",
    "    if 'error' not in best_result:\n",
    "        print(f\"Best F1 Score: {best_result.get('eval_f1', 0):.3f}\")\n",
    "        print(f\"Best Accuracy: {best_result.get('eval_accuracy', 0):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualization\n",
    "valid_results = {k: v for k, v in results.items() if 'error' not in v}\n",
    "\n",
    "if valid_results:\n",
    "    models = list(valid_results.keys())\n",
    "    f1_scores = [valid_results[model].get('eval_f1', 0) for model in models]\n",
    "    accuracies = [valid_results[model].get('eval_accuracy', 0) for model in models]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # F1 Score comparison\n",
    "    ax1.bar(models, f1_scores, color='skyblue')\n",
    "    ax1.set_title('F1 Score Comparison')\n",
    "    ax1.set_ylabel('F1 Score')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Accuracy comparison\n",
    "    ax2.bar(models, accuracies, color='lightgreen')\n",
    "    ax2.set_title('Accuracy Comparison')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No valid results to visualize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed evaluation with NERModelEvaluator\n",
    "evaluator = NERModelEvaluator()\n",
    "\n",
    "# Create mock predictions for demonstration\n",
    "true_labels = eval_labels[:5]  # Use first 5 sentences\n",
    "pred_labels = eval_labels[:5]  # Mock predictions (same as true for demo)\n",
    "\n",
    "# Evaluate\n",
    "detailed_results = evaluator.evaluate_predictions(true_labels, pred_labels, best_model or \"XLM-RoBERTa\")\n",
    "\n",
    "print(\"Detailed Evaluation Results:\")\n",
    "print(f\"Overall F1: {detailed_results['overall_metrics']['f1_score']:.3f}\")\n",
    "print(f\"Overall Precision: {detailed_results['overall_metrics']['precision']:.3f}\")\n",
    "print(f\"Overall Recall: {detailed_results['overall_metrics']['recall']:.3f}\")\n",
    "\n",
    "print(\"\\nEntity-level Performance:\")\n",
    "for entity, metrics in detailed_results['entity_metrics'].items():\n",
    "    print(f\"  {entity}: F1={metrics['f1_score']:.3f}, P={metrics['precision']:.3f}, R={metrics['recall']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model selection summary\n",
    "print(\"Model Selection Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Models evaluated: {len(results)}\")\n",
    "print(f\"Best model: {best_model}\")\n",
    "if best_model and best_model in results and 'error' not in results[best_model]:\n",
    "    print(f\"Best F1 score: {results[best_model].get('eval_f1', 0):.3f}\")\n",
    "print(\"\\nRecommendation: Use the best performing model for production deployment\")\n",
    "print(f\"Model path: ../models/checkpoints/{best_model.replace('/', '_') if best_model else 'xlm-roberta-base'}/final_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}