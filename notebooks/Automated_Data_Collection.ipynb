{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚è∞ Automated Telegram Data Collection\n",
    "\n",
    "## Overview\n",
    "Automated scheduling system for continuous data collection:\n",
    "- **Scheduled scraping** (hourly, daily, weekly)\n",
    "- **Incremental updates** (only new messages)\n",
    "- **Error handling** and retry logic\n",
    "- **Data deduplication** and validation\n",
    "- **Background execution** with logging\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìö Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import asyncio\n",
    "import schedule\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "from threading import Thread\n",
    "\n",
    "# Add scripts to path\n",
    "sys.path.append(os.path.abspath('../src/data_collection'))\n",
    "from telegram_scraper import TelegramScraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scheduling configuration\n",
    "SCHEDULE_INTERVAL = \"daily\"  # Options: \"hourly\", \"daily\", \"weekly\"\n",
    "SCHEDULE_TIME = \"09:00\"      # Time for daily/weekly runs\n",
    "\n",
    "# Data collection settings\n",
    "CHANNELS = [\n",
    "    '@classybrands',\n",
    "    '@Shageronlinestore', \n",
    "    '@ZemenExpress',\n",
    "    '@sinayelj',\n",
    "    '@modernshoppingcenter'\n",
    "]\n",
    "\n",
    "OUTPUT_FILE = '../data/telegram_data.csv'\n",
    "LOG_FILE = '../logs/scraper.log'\n",
    "INCREMENTAL_LIMIT = 100  # New messages per run\n",
    "\n",
    "# Setup logging\n",
    "os.makedirs('../logs', exist_ok=True)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(LOG_FILE),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"‚è∞ Schedule: {SCHEDULE_INTERVAL} at {SCHEDULE_TIME}\")\n",
    "print(f\"üì° Channels: {len(CHANNELS)}\")\n",
    "print(f\"üìù Log file: {LOG_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîß Scraping Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutomatedScraper:\n",
    "    def __init__(self):\n",
    "        self.api_id = os.getenv('TG_API_ID')\n",
    "        self.api_hash = os.getenv('TG_API_HASH')\n",
    "        self.scraper = None\n",
    "        \n",
    "    async def incremental_scrape(self):\n",
    "        \"\"\"Scrape only new messages since last run\"\"\"\n",
    "        try:\n",
    "            logging.info(\"Starting incremental scrape...\")\n",
    "            \n",
    "            # Initialize scraper\n",
    "            if not self.scraper:\n",
    "                self.scraper = TelegramScraper(self.api_id, self.api_hash)\n",
    "            \n",
    "            # Get last message timestamp\n",
    "            last_timestamp = self.get_last_timestamp()\n",
    "            \n",
    "            # Scrape new messages\n",
    "            new_data = await self.scraper.scrape_channels(\n",
    "                channels=CHANNELS,\n",
    "                limit=INCREMENTAL_LIMIT,\n",
    "                since_date=last_timestamp\n",
    "            )\n",
    "            \n",
    "            # Save new data\n",
    "            if new_data:\n",
    "                self.append_data(new_data)\n",
    "                logging.info(f\"Collected {len(new_data)} new messages\")\n",
    "            else:\n",
    "                logging.info(\"No new messages found\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Scraping failed: {str(e)}\")\n",
    "            \n",
    "    def get_last_timestamp(self):\n",
    "        \"\"\"Get timestamp of last collected message\"\"\"\n",
    "        try:\n",
    "            if os.path.exists(OUTPUT_FILE):\n",
    "                df = pd.read_csv(OUTPUT_FILE)\n",
    "                if not df.empty:\n",
    "                    return pd.to_datetime(df['Date']).max()\n",
    "        except:\n",
    "            pass\n",
    "        return datetime.now() - timedelta(days=1)\n",
    "    \n",
    "    def append_data(self, new_data):\n",
    "        \"\"\"Append new data to existing file\"\"\"\n",
    "        new_df = pd.DataFrame(new_data)\n",
    "        \n",
    "        if os.path.exists(OUTPUT_FILE):\n",
    "            existing_df = pd.read_csv(OUTPUT_FILE)\n",
    "            combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "            # Remove duplicates based on message ID\n",
    "            combined_df = combined_df.drop_duplicates(subset=['ID'], keep='last')\n",
    "        else:\n",
    "            combined_df = new_df\n",
    "            \n",
    "        combined_df.to_csv(OUTPUT_FILE, index=False, encoding='utf-8')\n",
    "        logging.info(f\"Data saved to {OUTPUT_FILE}\")\n",
    "\n",
    "# Initialize scraper\n",
    "auto_scraper = AutomatedScraper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚è∞ Schedule Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_scraper():\n",
    "    \"\"\"Wrapper to run async scraper in sync context\"\"\"\n",
    "    try:\n",
    "        loop = asyncio.new_event_loop()\n",
    "        asyncio.set_event_loop(loop)\n",
    "        loop.run_until_complete(auto_scraper.incremental_scrape())\n",
    "        loop.close()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Scheduler error: {str(e)}\")\n",
    "\n",
    "# Setup schedule based on configuration\n",
    "if SCHEDULE_INTERVAL == \"hourly\":\n",
    "    schedule.every().hour.do(run_scraper)\n",
    "    print(\"üìÖ Scheduled: Every hour\")\n",
    "elif SCHEDULE_INTERVAL == \"daily\":\n",
    "    schedule.every().day.at(SCHEDULE_TIME).do(run_scraper)\n",
    "    print(f\"üìÖ Scheduled: Daily at {SCHEDULE_TIME}\")\n",
    "elif SCHEDULE_INTERVAL == \"weekly\":\n",
    "    schedule.every().monday.at(SCHEDULE_TIME).do(run_scraper)\n",
    "    print(f\"üìÖ Scheduled: Weekly on Monday at {SCHEDULE_TIME}\")\n",
    "\n",
    "print(f\"‚è∞ Next run: {schedule.next_run()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üöÄ Start Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_scheduler():\n",
    "    \"\"\"Run the scheduler in background\"\"\"\n",
    "    logging.info(\"Scheduler started\")\n",
    "    while True:\n",
    "        schedule.run_pending()\n",
    "        time.sleep(60)  # Check every minute\n",
    "\n",
    "# Start scheduler in background thread\n",
    "scheduler_thread = Thread(target=run_scheduler, daemon=True)\n",
    "scheduler_thread.start()\n",
    "\n",
    "print(\"‚úÖ Automated scraper started!\")\n",
    "print(\"üìä Monitor logs for scraping activity\")\n",
    "print(\"‚èπÔ∏è Run next cell to stop scheduler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß™ Manual Test Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the scraper manually\n",
    "print(\"üß™ Running test scrape...\")\n",
    "await auto_scraper.incremental_scrape()\n",
    "print(\"‚úÖ Test completed - check logs for results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Monitor Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current status\n",
    "def show_status():\n",
    "    print(\"üìä Scraper Status:\")\n",
    "    print(f\"‚è∞ Next scheduled run: {schedule.next_run()}\")\n",
    "    \n",
    "    if os.path.exists(OUTPUT_FILE):\n",
    "        df = pd.read_csv(OUTPUT_FILE)\n",
    "        print(f\"üìà Total messages: {len(df):,}\")\n",
    "        print(f\"üìÖ Latest message: {df['Date'].max()}\")\n",
    "        print(f\"üì° Active channels: {df['Channel Username'].nunique()}\")\n",
    "    else:\n",
    "        print(\"üìÇ No data file found yet\")\n",
    "    \n",
    "    # Show recent log entries\n",
    "    if os.path.exists(LOG_FILE):\n",
    "        print(\"\\nüìù Recent log entries:\")\n",
    "        with open(LOG_FILE, 'r') as f:\n",
    "            lines = f.readlines()[-5:]  # Last 5 lines\n",
    "            for line in lines:\n",
    "                print(f\"  {line.strip()}\")\n",
    "\n",
    "show_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚èπÔ∏è Stop Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the scheduler (run this cell to stop)\n",
    "schedule.clear()\n",
    "print(\"‚èπÔ∏è Scheduler stopped\")\n",
    "print(\"üìä Final status:\")\n",
    "show_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîß Production Deployment\n",
    "\n",
    "For production deployment, create a standalone script:\n",
    "\n",
    "```python\n",
    "# save as: automated_scraper.py\n",
    "import schedule\n",
    "import time\n",
    "from automated_scraper import AutomatedScraper\n",
    "\n",
    "scraper = AutomatedScraper()\n",
    "schedule.every().day.at(\"09:00\").do(scraper.run)\n",
    "\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(60)\n",
    "```\n",
    "\n",
    "**Run with:**\n",
    "```bash\n",
    "# Background process\n",
    "nohup python automated_scraper.py &\n",
    "\n",
    "# Or with systemd service\n",
    "sudo systemctl enable telegram-scraper\n",
    "sudo systemctl start telegram-scraper\n",
    "```\n",
    "\n",
    "**Features:**\n",
    "- ‚úÖ Incremental data collection\n",
    "- ‚úÖ Automatic deduplication\n",
    "- ‚úÖ Error handling and logging\n",
    "- ‚úÖ Configurable scheduling\n",
    "- ‚úÖ Background execution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}